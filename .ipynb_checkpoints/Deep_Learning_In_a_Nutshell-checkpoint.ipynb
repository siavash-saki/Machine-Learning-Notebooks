{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "![perceptron](notebook_data/perceptron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron model (MLN) (Feed-Forward Neural Networks)\n",
    "* Feedforward neural networks are artificial neural networks (ANN)\n",
    "* No Feedback Connections (connections between units do not form a cycle)\n",
    "* called feedforward because information only travels forward in the network (no loops), first through the input nodes, then through the hidden nodes (if present), and finally through the output nodes\n",
    "* primarily used for supervised learning in cases where the data to be learned is neither sequential nor time-dependent\n",
    "\n",
    "![Neural Networks](notebook_data/neural_networks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks\n",
    "**Neural Network with 2 or more hidden layers** \n",
    "![Neural Networks](notebook_data/deep_neural_networks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Approximation Theorem\n",
    "In the mathematical theory of artificial neural networks, the **universal approximation theorem** states that a **feed-forward network with a single hidden layer containing a finite number of neurons** can approximate any convex continuous function on compact subsets of Rn, under mild assumptions on the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "[Wikipedia Page](https://en.wikipedia.org/wiki/Activation_function)\n",
    "\n",
    "Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron’s input is relevant for the model’s prediction. Activation functions also help normalize the output of each neuron to a range between 1 and 0 or between -1 and 1.\n",
    "\n",
    "\n",
    "**Most popular types of Activation functions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sigmoid or Logistic\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Activation_logistic.svg/120px-Activation_logistic.svg.png\" align=\"left\"/>\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/36f792c44c0a7069ad01386452569d6e34fe95d7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tanh - Hyperbolic tangent\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Activation_tanh.svg/120px-Activation_tanh.svg.png\" align=\"left\"/>\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/84c428bf21e34ccc0be8becf3443b06a4b61f3ee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ReLu - Rectified linear unit\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/120px-Activation_rectified_linear.svg.png\" align=\"left\"/>\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/824a1cc623637e8a5c041a4ac3fc96aa70ed88ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Leaky ReLU - Leaky rectified linear unit\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Activation_prelu.svg/120px-Activation_prelu.svg.png\" align=\"left\"/>\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/aaabce8985d074b5f4482f4efa327c7c61da3ca6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification \n",
    "* **Non-Exclusive classes**\\\n",
    "A data point can have multiple classes (Photos labels like beach, sunny, people, ..)\\\n",
    "*Sigmoid Function*\n",
    "\n",
    "\n",
    "* **Mutualy Exclusive classes**\\\n",
    "Only one class per data point (Digits Recignitions)\\\n",
    "*Softmax Function*\n",
    "The sum of all the probabilities will be equal to one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function (Loss Function)\n",
    "\n",
    "[Further Details](https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications)\n",
    "\n",
    "![Network](notebook_data/network_notation.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A cost function is a function that maps values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a cost function. An objective function is either a loss function or its negative (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized.\n",
    "\n",
    "A cost function is a single value, not a vector, because it rates how good the neural network did as a whole. Specifically, a cost function is of the form:\n",
    "\n",
    "$C(W, B, S^r, E^r)$\n",
    "\n",
    "where \n",
    "- $W$ is the neural network's weights\n",
    "- $B$ is the neural network's biases\n",
    "- $S_r$ is the input of a single training sample\n",
    "- $E_r$ is the desired output of that training sample\n",
    "\n",
    "\n",
    "\n",
    "* **Quadratic cost**\n",
    "\n",
    "Also known as mean squared error, maximum likelihood, and sum squared error, this is defined as:\n",
    "\n",
    "$C_{MST}(W, B, S^r, E^r) = 0.5\\sum\\limits_j (a^L_j - E^r_j)^2$\\\n",
    "or\n",
    "\n",
    "$\\frac{1}{2n}\\sum\\limits_j ||y(x) - a(x)^L||^2$\n",
    "\n",
    "\n",
    "* **Cross-entropy cost**\n",
    "\n",
    "Also known as Bernoulli negative log-likelihood and Binary Cross-Entropy\n",
    "\n",
    "$C_{CE}(W, B, S^r, E^r) = -\\sum\\limits_j [E^r_j \\text{ ln } a^L_j + (1 - E^r_j) \\text{ ln }(1-a^L_j)]$\n",
    "\n",
    "* **Exponentional cost**\n",
    "\n",
    "This requires choosing some parameter $\\tau$ that you think will give you the behavior you want. Typically you'll just need to play with this until things work good.\n",
    "\n",
    "$C_{EXP}(W, B, S^r, E^r) = \\tau\\text{ }\\exp(\\frac{1}{\\tau} \\sum\\limits_j (a^L_j - E^r_j)^2)$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geadient Decent\n",
    "\n",
    "[Further Details](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms)\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.\n",
    "\n",
    "<img src=\"notebook_data/saddle_point_evaluation_optimizers.gif\" width=\"400\" align=\"left\"/>\n",
    "<img src=\"notebook_data/contours_evaluation_optimizers.gif\" width=\"400\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent optimization algorithms\n",
    "\n",
    "The common gradient descent optimization algorithms used in popular deep learning frameworks:\n",
    "\n",
    "\n",
    "1. Stochastic Gradient Descent (SGD)\n",
    "2. Momentum\n",
    "2. AdaGrad\n",
    "2. RMSprop\n",
    "2. Adadelta\n",
    "2. NAG\n",
    "2. Adam\n",
    "2. AdaMax\n",
    "2. Nadam\n",
    "2. AMSGrad\n",
    "\n",
    "\n",
    "[Further Details](https://towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "The size of these steps is called the learning rate. With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom.\n",
    "\n",
    "<img src=\"notebook_data/LearningRateTooLarge.svg\" width=\"600\" align=\"left\"/>\n",
    "<img src=\"notebook_data/LearningRateTooSmall.svg\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation (backprop, BP)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is a widely used algorithm in training feedforward neural networks for supervised learning. In deep learning, backpropagation aims to minimize the cost function by adjusting network’s weights and biases. The level of adjustment is determined by the gradients of the cost function with respect to those parameters.\n",
    "\n",
    "[How the backpropagation algorithm works](http://neuralnetworksanddeeplearning.com/chap2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Kernels (Image Filters)\n",
    "\n",
    "An image kernel is a small matrix used to apply effects like the ones you might find in Photoshop or Gimp, such as blurring, sharpening, outlining or embossing. They're also used in machine learning for 'feature extraction', a technique for determining the most important portions of an image. In this context the process is referred to more generally as \"convolution\"\n",
    "\n",
    "[Further details](http://setosa.io/ev/image-kernels/)\n",
    "\n",
    "![Image Kernel](notebook_data/image-kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architucture\n",
    "\n",
    "In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. The role of the ConvNet is to reduce the images into a form which is easier to process, without losing features which are critical for getting a good prediction.\n",
    "\n",
    "[Further Details](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "\n",
    "![CNN](notebook_data/CNN.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer (The Kernel)\n",
    "\n",
    "The element involved in carrying out the convolution operation in the first part of a Convolutional Layer is called the **Kernel/Filter**. The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image.\n",
    "\n",
    "There are two types of results to the operation — one in which the convolved feature is reduced in dimensionality as compared to the input, and the other in which the dimensionality is either increased or remains the same. This is done by applying **Valid Padding** in case of the former, or **Same Padding** in the case of the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grayscale Images 2D**\n",
    "\n",
    "Convoluting a 5x5x1 image with a 3x3x1 kernel to get a 3x3x1 convolved feature\n",
    "\n",
    "<img src=\"notebook_data/CNN-kernel.gif\" width=\"400\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Color Images 3d**\n",
    "\n",
    "Convolution operation on a MxNx3 image matrix with a 3x3x3 Kernel\n",
    "\n",
    "<img src=\"notebook_data/CNN-kernel-3D.gif\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride\n",
    "\n",
    "Stride is the number of pixels shifts over the input matrix. When the stride is 1 then we move the filters to 1 pixel at a time. When the stride is 2 then we move the filters to 2 pixels at a time and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu Layer\n",
    "\n",
    "Relu increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer.\n",
    "\n",
    "<img src=\"notebook_data/CNN-Relu.png\" align=\"left\"/>\n",
    "<img src=\"notebook_data/Relu.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "\n",
    "Similar to the Convolutional Layer, the Pooling layer is responsible for **reducing the spatial size of the Convolved Feature**. This is to **decrease the computational power required to process the data** through dimensionality reduction. Furthermore, it is useful for **extracting dominant features** which are rotational and positional invariant, thus maintaining the process of effectively training of the model.\n",
    "\n",
    "There are two types of Pooling: **Max Pooling** and **Average Pooling**. Max Pooling performs a lot better than Average Pooling.\n",
    "\n",
    "\n",
    "<img src=\"notebook_data/CNN-pooling.jpeg\" width=\"400\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer (FC Layer) - Classification\n",
    "\n",
    "The Fully-Connected layer is learning a possibly non-linear function in that space.he flattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Layer or Loss Layer\n",
    "\n",
    "Output the class using an activation function and classifies images.\n",
    "**Softmax loss** is used for predicting a single class of K mutually exclusive classes. **Sigmoid cross-entropy** loss is used for predicting K independent probability values in [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
