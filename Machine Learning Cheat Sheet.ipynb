{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Cheat Sheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Created On: Timestamp('2019-10-29 15:37:20.527174')\n",
    "    Created By: 'Siavash Saki'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chosing the best algorithm\n",
    "          \n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_static/ml_map.png' width=500 align=\"middle\" />\n",
    "\n",
    "[Full Size Image](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the data\n",
    "```python\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe()\n",
    "sns.pairplot(df)\n",
    "\n",
    "df.corr()\n",
    "sns.heatmap(df.corr())\n",
    "\n",
    "sns.distplot(df[TARGET]) \n",
    "sns.countplot(df[TARGET]) #categorical\n",
    "\n",
    "#two hist on one fig\n",
    "sns.distplot(a=df[df['male']==1]['Survived'],kde=False)\n",
    "sns.distplot(a=df[df['male']==0]['Survived'],kde=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with missing data\n",
    "```python\n",
    "df.isnull().sum()\n",
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Regression Test\n",
    "```python\n",
    "sns.regplot(x='feature',y='target',data=df)\n",
    "sns.residplot(x='feature',y='target',data=df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hote Encoding\n",
    "```python\n",
    "# Making dummy variable\n",
    "cat_dummy = pd.get_dummies(df['category_i'],drop_first=True)\n",
    "df=pd.concat([df,cat_dummy],axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data into train and test set\n",
    "```python\n",
    "X= df.drop('TARGET',axis=1)\n",
    "y= df[TARGET]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Time Series\n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "ts= TimeSeriesSplit(n_splits=4)\n",
    "cross_val_score(estimator,X,y,cv=ts)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression line evaluation\n",
    "```python\n",
    "from scipy import stats\n",
    "pearson_coef, p_value = stats.pearsonr(df['wheel-base'], df['price'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANOVA\n",
    "```python\n",
    "from scipy import stats\n",
    "F, p = stats.f_oneway(group_A, group_B, group_C)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling Data\n",
    "\n",
    "*MinMaxScaler:*\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "scaled = min_max_scaler.fit_transform(x)\n",
    "```\n",
    "*StandardScaler:*\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "ss= StandardScaler()\n",
    "ss.fit(features)\n",
    "scaled= ss.transform(features)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal component analysis (PCA)\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "pca= PCA(n_components=2)\n",
    "pca.fit(scaled)\n",
    "x_pca= pca.transform(scaled)\n",
    "\n",
    "pca_components= pd.DataFrame(x_pca,columns=['First principal component','Second principal component'])\n",
    "pca_data= pd.concat([pca_components,df['target']],axis=1)\n",
    "sns.scatterplot(x='First principal component',y='Second principal component',hue='target',data=pca_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation\n",
    "\n",
    "for example the estimator is a support vector machines:\n",
    "`svc= sklearn.svm.SVC()`\n",
    "\n",
    "*one random train-test set:*\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "svc.fit(X_train,y_train)\n",
    "svc.score(X_test,y_test)\n",
    "\n",
    "\n",
    "```\n",
    "*n random train-test set:*\n",
    "```python\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv= ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "score= cross_val_score(svc, X,y, cv=cv)\n",
    "print(score)\n",
    "print(score.mean())\n",
    "\n",
    "# For Regression:\n",
    "r2_score= cross_val_score(lr, X,y, cv=cv)\n",
    "MSE= -1 * cross_val_score(lr,X,y,cv=cv,scoring='neg_mean_squared_error')\n",
    "```\n",
    "\n",
    "*cross validation {cv is by default: \"(Stratified)KFold\"}:*\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "y_hat= cross_val_predict(svc, X,y, cv=10)\n",
    "score= cross_val_score(svc, X,y, cv=10)\n",
    "print(score)\n",
    "print(score.mean())\n",
    "\n",
    "# For Regression:\n",
    "r2_score= cross_val_score(lr, X,y, cv=5)\n",
    "MSE= -1 * cross_val_score(lr,X,y,cv=5,scoring='neg_mean_squared_error')\n",
    "```\n",
    "\n",
    "*K Fold:*\n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "kf = KFold(n_splits=4,shuffle=True)\n",
    "score= cross_val_score(svc, X,y, cv=kf)\n",
    "print(score)\n",
    "print(score.mean())\n",
    "# kf.split(X) # indexes in train and test set\n",
    "\n",
    "# For Regression: analog to cv\n",
    "```\n",
    "\n",
    "*Stratified K Fold (evenly distrubted traget class in train and test sets):*\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "skf= StratifiedKFold(n_splits=4,shuffle=True)\n",
    "score= cross_val_score(svc, X,y, cv=skf)\n",
    "print(score)\n",
    "print(score.mean())\n",
    "# skf.split(X,y) # indexes in train and test set\n",
    "\n",
    "# For Regression: analog to cv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid={'C':[0.1,1,10,100,1000],'gamma':[1,0.1,0.001,0.0001,0.00001]}\n",
    "gs= GridSearchCV(estimator=SVC(),param_grid=param_grid,verbose=2)\n",
    "gs.fit(X_train,y_train)\n",
    "gs.best_estimator_\n",
    "gs.best_params_\n",
    "gs.cv_results_\n",
    "y_hat= gs.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation (classifier models)\n",
    "\n",
    "*Jaccard index:*\n",
    "```python\n",
    "from sklearn.metrics import jaccard_similarity_score #depricated instead: jaccard_score\n",
    "jaccard_similarity_score(y_test, y_hat)\n",
    "```\n",
    "\n",
    "\n",
    "*log loss:*\n",
    "\n",
    "```python\n",
    "# For logistic regression\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_hat_prob)\n",
    "```\n",
    "\n",
    "\n",
    "*f1-score:*\n",
    "```python\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "print(accuracy_score(y_test,y_hat),'\\n',\n",
    "      confusion_matrix(y_test,y_hat),'\\n',\n",
    "      classification_report(y_test,y_hat))\n",
    "```\n",
    "For more details:\n",
    "* **[precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)** \n",
    "* **[f1-score](https://en.wikipedia.org/wiki/F1_score)**\\\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png' width=300 align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and loading objects and using pickle\n",
    "```python\n",
    "import pickle\n",
    "# save an object\n",
    "with open('file_name.pkl', 'wb') as fp:\n",
    "    pickle.dump(my_object, fp)\n",
    "# load an object\n",
    "with open('file_name.pkl', 'rb') as fp:\n",
    "    my_object = peakle.load(fp)\n",
    "# faster way for data frames\n",
    "df.to_pickle('file_name.pkl')\n",
    "df= pd.read_pickle('file_name.pkl)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model on new data\n",
    "```python\n",
    "estimator.set_params(warm_start=True)\n",
    "estimator.fit(X_new,y_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least Squares Regression:\\\n",
    "$min\\ (sum\\ of\\ the\\ squared\\ residuals)$\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm= LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "lm.intercept_\n",
    "lm.coef_\n",
    "pd.concat([pd.DataFrame(lm.coef_.transpose(),index=X_train.columns,columns=['coef']),\n",
    "           pd.DataFrame(lm.intercept_,index=['y_intercept'],columns=['coef'])])\n",
    "\n",
    "y_hat= lm.predict(X_test)\n",
    "sns.scatterplot(y_test,y_hat)\n",
    "sns.distplot(y_test-y_pred).set_title('Residuals')\n",
    "lm.score(X_train,y_train)\n",
    "lm.score(X_test,y_test)== metrics.r2_score(y_test,y_hat)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.mean_absolute_error(y_test,y_hat)\n",
    "metrics.mean_squared_error(y_test,y_hat)\n",
    "np.sqrt(metrics.mean_squared_error(y_test,y_hat))\n",
    "metrics.explained_variance_score(y_test,y_hat)\n",
    "metrics.r2_score(y_test,y_hat)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression methods:\n",
    "* add bias and reduce variance (optimum fit: min total error)\n",
    "* reduce model complexity\n",
    "* prevent over-fitting which may result from simple linear regression\n",
    "* scale your data before using regularized linear regression methods (Standard Scaler) (Or \"normalize\"=True)\\\n",
    "<img src='https://miro.medium.com/max/481/1*cB0ESE9z3rB3-rpXPhwgWw.png' width=400 align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$min\\ (sum\\ of\\ the\\ squared\\ residuals\\ +\\ \\alpha * slope^{2})$\\\n",
    "Ridge Regression can reduce the slope **close to zero**.\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "# from sklearn.linear_model import RidgeCV # with cross-validation\n",
    "\n",
    "rr = Ridge(alpha=0.01)\n",
    "rr.fit(X_train, y_train)\n",
    "rr.score(X_test, y_test)\n",
    "mean_squared_error(y_test, rr.predict(X_test))\n",
    "# Using GridSearch, we can tune alpha {'alpha':[0.0001,0.001,0.1,1,10,100], 'normalize':[True, False]}\n",
    "           \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$min\\ (sum\\ of\\ the\\ squared\\ residuals\\ +\\ \\alpha * |slope|)$\\\n",
    "Lasso Regression can reduce the slope **to zero**. (Feature Elimination)\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "# from sklearn.linear_model import LassoCV # with cross-validation\n",
    "\n",
    "ls = Lasso(alpha=0.01)\n",
    "ls.fit(X_train, y_train)\n",
    "ls.score(X_test, y_test)\n",
    "mean_squared_error(y_test, ls.predict(X_test))\n",
    "# Using GridSearch, we can tune alpha {'alpha':[0.0001,0.001,0.1,1,10,100], 'normalize':[True, False]}\n",
    "           \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Elastic Net Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hybrid of Lasso and Ridge:\\\n",
    "$min\\ (sum\\ of\\ the\\ squared\\ residuals\\ +\\ \\alpha_{1} * |slope| +\\ \\alpha_{2} * slope^{2})$\n",
    "\n",
    "\n",
    "When the l1_ratio is set to 0 it is the same as ridge regression. When l1_ratio is set to 1 it is lasso. Elastic net is somewhere between 0 and 1.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# from sklearn.linear_model import ElasticNetCV # with cross-validation\n",
    "\n",
    "en= ElasticNet()\n",
    "params={'alpha':[0.001,0.01,0.1,1,10,100],\n",
    "        'l1_ratio':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "        'normalize':[True, False]}\n",
    "gs= GridSearchCV(en,param_grid=params,cv=5)\n",
    "gs.fit(X,y)\n",
    "gs.best_score_\n",
    "gs.best_params_\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# In case there's only one independent variable (x), we have to reshape the array\n",
    "# If there's a data frame, it is normal\n",
    "\n",
    "X= df['x'].values\n",
    "y= df['y'].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf= PolynomialFeatures(degree=2)\n",
    "X_poly= pf.fit_transform(X_train.reshape(-1,1))\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.fit(X_poly,y_train)\n",
    "\n",
    "# y_hat= lm.predict(X_poly)\n",
    "X_poly_test= pf.fit_transform(X_test.reshape(-1,1))\n",
    "y_hat_test= lm.predict(X_poly_test)\n",
    "\n",
    "# simple visualisation to avoid overfitting\n",
    "r2_train=[]\n",
    "r2_test=[]\n",
    "for i in range(1,11):\n",
    "    pf= PolynomialFeatures(degree=i)\n",
    "    X_poly= pf.fit_transform(X_train.reshape(-1,1))\n",
    "    lm=LinearRegression()\n",
    "    lm.fit(X_poly,y_train)\n",
    "    y_hat= lm.predict(X_poly)\n",
    "    X_poly_test= pf.fit_transform(X_test.reshape(-1,1))\n",
    "    y_hat_test= lm.predict(X_poly_test)\n",
    "    r2_train.append(r2_score(y_train,y_hat))\n",
    "    r2_test.append(r2_score(y_test,y_hat_test))\n",
    "plt.plot(range(1,11),r2_train,'--ro')\n",
    "plt.plot(range(1,11),r2_test,'--bo')\n",
    "\n",
    "# If it is data frame, we can simply use a pipeline:\n",
    "from sklearn.pipeline import Pipeline\n",
    "pl= Pipeline([('scale',StandardScaler()), #if necessary\n",
    "              ('pr',PolynomialFeatures(degree=2)),\n",
    "              ('lm',LinearRegression())])\n",
    "pl.fit(X_train,y_train)\n",
    "y_hat= pl.predict(X_test)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logm= LogisticRegression()\n",
    "logm.fit(X_train,y_train)\n",
    "\n",
    "logm.intercept_\n",
    "logm.coef_\n",
    "pd.concat([pd.DataFrame(logm.coef_.transpose(),index=X_train.columns,columns=['coef']),\n",
    "           pd.DataFrame(logm.intercept_,index=['y_intercept'],columns=['coef'])])\n",
    "\n",
    "y_hat= logm.predict(X_test)\n",
    "y_probability= logm.predict_proba(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# scaling the data if necessary\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar= StandardScaler()\n",
    "scalar.fit(df.drop('TARGET CLASS',axis=1))\n",
    "scaled_ar= scalar.transform(df.drop('TARGET CLASS',axis=1))\n",
    "scaled_df= pd.DataFrame(scaled_ar,columns=df.columns[:-1])\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn= KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train,y_train)\n",
    "y_hat= knn.predict(X_test)\n",
    "\n",
    "a_scores=[]\n",
    "err=[]\n",
    "for i in range(1,50):\n",
    "    knn_i= KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_i.fit(X_train,y_train)\n",
    "    y_hat_i= knn_i.predict(X_test)\n",
    "    a_scores.append(metrics.accuracy_score(y_test,y_hat_i))\n",
    "    err.append(np.mean(y_hat_i != y_test))\n",
    "\n",
    "plt.plot(err,'--ro')\n",
    "plt.title('Error Rate vs. K Value')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc= DecisionTreeClassifier()\n",
    "dtc.fit(X_train,y_train)\n",
    "y_hat= dtc.predict(X_test)\n",
    "```\n",
    "#### Tree Visualization\n",
    "```python\n",
    "#graphviz is needed\n",
    "#sudo apt-get install graphviz\n",
    "from IPython.display import Image  \n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot \n",
    "features = list(df.columns[:-1])\n",
    "dot_data = StringIO()  \n",
    "export_graphviz(dtc, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph[0].create_png())  \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "rfc.fit(X_train,y_train)\n",
    "y_hat= rfc.predict(X_test)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.svm import SVC\n",
    "svm= SVC()\n",
    "svm.fit(X_train,y_train)\n",
    "y_hat= svm.predict(X_test)\n",
    "\n",
    "#Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid={'kernel':['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "            'degree':[2,3,4], # Degree of the polynomial kernel function ('poly')\n",
    "            'C':[0.1,1,10,100,1000],\n",
    "            'gamma':[1,0.1,0.001,0.0001,0.00001]}\n",
    "gs= GridSearchCV(estimator=SVC(),param_grid=param_grid,verbose=2)\n",
    "gs.fit(X_train,y_train)\n",
    "gs.best_estimator_\n",
    "gs.best_params_\n",
    "y_hat= gs.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "kmc= KMeans(n_clusters=2)\n",
    "kmc.fit(data)\n",
    "\n",
    "kmc.cluster_centers_\n",
    "kmc.labels_\n",
    "\n",
    "#Elbow methode\n",
    "wcss=[]\n",
    "for i in range(1,16):\n",
    "    kmc= KMeans(n_clusters=i)\n",
    "    kmc.fit(features)\n",
    "    wcss.append(kmc.inertia_)\n",
    "plt.plot(list(range(1,16)),wcss,'--ro')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Within cluster sum of squares')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering \n",
    "\n",
    "```python\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "ac= AgglomerativeClustering(n_clusters=3,affinity='euclidean',linkage='average')\n",
    "y_hat= ac.fit_predict(X)\n",
    "\n",
    "# Dendrogram Associated for the Agglomerative Hierarchical Clustering\n",
    "from scipy.cluster import hierarchy \n",
    "from scipy.spatial import distance_matrix \n",
    "dist_matrix = distance_matrix(X,X) \n",
    "Z = hierarchy.linkage(dist_matrix, 'complete')\n",
    "dendro = hierarchy.dendrogram(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density-Based Clustering (DBSCAN)\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN \n",
    "epsilon = 3.6\n",
    "minimumSamples = 10\n",
    "db = DBSCAN(eps=epsilon, min_samples=minimumSamples).fit(X)\n",
    "labels = db.labels_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# example:\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline= Pipeline([('bow', CountVectorizer(stop_words=stopwords.words('english'))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('mnb', MultinomialNB()) ])\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_hat= pipeline.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
