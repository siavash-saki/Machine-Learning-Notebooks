{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents:\n",
    "\n",
    "[**Spark Dataframes**](#Spark-Dataframes)\n",
    "\n",
    "[**Machine Learning**](ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creat a Saprk Session\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Files\n",
    "```python\n",
    "df = spark.read.json('file.json')\n",
    "df = spark.read.csv('file.csv',inferSchema=True,header=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the schema manually when reading a file\n",
    "```python\n",
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType\n",
    "data_schema = [StructField(\"column1\", IntegerType(), True),\n",
    "               StructField(\"column2\", StringType(), True)]\n",
    "final_struc = StructType(fields=data_schema)\n",
    "df = spark.read.json('people.json', schema=final_struc)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Data Exploration \n",
    "```python\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.columns\n",
    "df.describe().show()\n",
    "df.select('column1').show()\n",
    "df.select(['column1','column2']).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grab data\n",
    "```python\n",
    "# Output is a list of ROWs\n",
    "df.collect()\n",
    "df.describe().collect()\n",
    "df.select('column1').collect()\n",
    "df.head(5)\n",
    "\n",
    "# we can use indexing by ROW object\n",
    "row = df.collect()[0]\n",
    "val0= row[0]\n",
    "# or turn them into dic\n",
    "val0= row.asDict()['column1']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and rename columns\n",
    "```python\n",
    "df.withColumn('new_column',df['column']*2).show()\n",
    "df.withColumnRenamed('column','column_new_name').show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SQL\n",
    "```python\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"table_name\")\n",
    "spark.sql(\"SQL QUERY HERE\").collect()\n",
    "spark.sql(\"\"\"\n",
    "SQL QUERY HERE\n",
    "\"\"\").show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Data\n",
    "```python\n",
    "df.filter(\"column1<500\").show()\n",
    "df.filter(\"column1<500\").select(['column1','column2']).show()\n",
    "df.filter(df[\"column1\"]<500).show()\n",
    "df.filter( (df[\"Column1\"]<200) & ~(df['Column2']<200) ).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Functions\n",
    "```python\n",
    "from pyspark.sql.functions import (countDistinct, \n",
    "                                   mean,\n",
    "                                   avg,\n",
    "                                   stddev,\n",
    "                                   format_number)\n",
    "\n",
    "from pyspark.sql.functions import (dayofmonth,\n",
    "                                   hour,\n",
    "                                   dayofyear,\n",
    "                                   month,\n",
    "                                   year,\n",
    "                                   weekofyear,\n",
    "                                   date_format)\n",
    "# Use Funcs\n",
    "df.select(mean(\"Column1\").alias(\"mean_col1\")).show()\n",
    "df.agg({'Column1':'mean'}).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with missing values\n",
    "```python\n",
    "# Dropping NON-Values\n",
    "df.na.drop().show() # Drop any row that contains missing data\n",
    "df.na.drop(thresh=2).show() # at least 2 NON-null values in the row\n",
    "df.na.drop(subset=[\"column1\"]).show() # Drop any row that has a NON-Value for column1\n",
    "\n",
    "# Filling NON-Values\n",
    "df.na.fill('new_value',subset=['Column1']).show() # Fill NA Values in column1\n",
    "mean_val = df.select(mean(df['Column1'])).collect()[0][0] # Fill NA Values in column1 with mean\n",
    "df.na.fill(mean_val,[\"Column1\"]).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group By\n",
    "```python\n",
    "df.groupBy(\"Column1\").sum().show()\n",
    "df.groupBy(\"Column1\").agg({\"Column2\":'stddev'}).show()\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order By\n",
    "```python\n",
    "# Ascending\n",
    "df.orderBy(\"column1\").show()\n",
    "# Descending\n",
    "df.orderBy(df[\"column1\"].desc()).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "```python\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "```python\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "```python\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "```python\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "```python\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "```python\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "```python\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
